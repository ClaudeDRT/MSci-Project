{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated_Echo_SVC\n",
    "## Claude de Rijke-Thomas\n",
    "### 15th March 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open in tensorflowenv: source activate tensorflowenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflowenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.integrate import simps, trapz\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, adam\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from numba import jit, prange\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP1 corresponds to:\n",
    "$$h\\_s = [0, 0.05, 0.10, 0.15] $$\n",
    "$$l\\_surf = [0:1:10]; % large-scale correlation length (default = 5 m) $$\n",
    "$$sigma\\_si = [0.001:0.0005:0.004]; % sea ice rms height (default = 0.002 m) $$\n",
    "$$sigma\\_surf = [0:0.05:0.5]; % large-scale rms roughness height (default = 0.1 m)$$\n",
    "NLP2 corresponds to:\n",
    "$$h\\_s =  [0.20, 0.25, 0.30, 0.35]$$\n",
    "$$l\\_surf = [0:1:10]; % large-scale correlation length (default = 5 m) $$\n",
    "$$sigma\\_si = [0.001:0.0005:0.004]; % sea ice rms height (default = 0.002 m) $$\n",
    "$$sigma\\_surf = [0:0.05:0.5]; % large-scale rms roughness height (default = 0.1 m)$$\n",
    "NLP3 corresponds to:\n",
    "$$h\\_s =  [0.40, 0.45, 0.50]$$\n",
    "$$l\\_surf = [0:1:10]; % large-scale correlation length (default = 5 m) $$\n",
    "$$sigma\\_si = [0.001:0.0005:0.004]; % sea ice rms height (default = 0.002 m) $$\n",
    "$$sigma\\_surf = [0:0.05:0.5]; % large-scale rms roughness height (default = 0.1 m)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Had to split the non-lead echo simulations into three parts: NLP1, NLP2 and NLP3\n",
    "NLP1 = scipy.io.loadmat('/Users/claudederijkethomas/Desktop/Ice/Proj/FEM_Sim_NLP1.mat')\n",
    "NLP2 = scipy.io.loadmat('/Users/claudederijkethomas/Desktop/Ice/Proj/FEM_Sim_NLP2.mat')\n",
    "NLP3 = scipy.io.loadmat('/Users/claudederijkethomas/Desktop/Ice/Proj/FEM_Sim_NLP3.mat')\n",
    "\n",
    "sim_params = NLP1['parameters_lookup']\n",
    "sim_params_2 = NLP2['parameters_lookup']\n",
    "sim_params_3 = NLP3['parameters_lookup']\n",
    "\n",
    "sim_echoes = NLP1['P_t_ml_range']\n",
    "sim_echoes_2 = NLP2['P_t_ml_range']\n",
    "sim_echoes_3 = NLP3['P_t_ml_range']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNLP1():\n",
    "    counter = 0\n",
    "    data_arr = []\n",
    "    label_arr = []\n",
    "    int_label_arr = []\n",
    "    for i in range(len(sim_echoes[:][:][:][:])): #snow depth{s} (h_s)\n",
    "        for j in range(len(sim_echoes[0][:][:][:])): #l_surf{s}\n",
    "            for k in range(len(sim_echoes[0][0][:][:])): #sigma_si{s}\n",
    "                for l in range(len(sim_echoes[0][0][0][:])): #sigma_surf{s}\n",
    "                    simulated_echo = [sim_echoes[i][j][k][l][m][0] for m in range(len(sim_echoes[i][j][k][l]))]\n",
    "                    sim_echo_params = [sim_params[i][j][k][l][0][m] for m in range(len(sim_params[i][j][k][l][0]))]\n",
    "                    simulated_echo_area = trapz(simulated_echo, dx = 1) \n",
    "                    norm_sim_echo = [x/simulated_echo_area for x in simulated_echo]\n",
    "                    data_arr.append(norm_sim_echo)\n",
    "                    label_arr.append(str(sim_echo_params))\n",
    "                    int_label_arr.append(counter)#+j*range(len(sim_echoes[0][:][:][:])) k*range(len(sim_echoes[0][0][:][:]))+l*range(len(sim_echoes[0][0][0][:]))))\n",
    "                    counter+=1\n",
    "    for i in range(len(sim_echoes_2[:][:][:][:])): #snow depth{s} (h_s)\n",
    "        for j in range(len(sim_echoes_2[0][:][:][:])): #l_surf{s}\n",
    "            for k in range(len(sim_echoes_2[0][0][:][:])): #sigma_si{s}\n",
    "                for l in range(len(sim_echoes_2[0][0][0][:])): #sigma_surf{s}\n",
    "                    simulated_echo = [sim_echoes_2[i][j][k][l][m][0] for m in range(len(sim_echoes_2[i][j][k][l]))]\n",
    "                    sim_echo_params = [sim_params_2[i][j][k][l][0][m] for m in range(len(sim_params_2[i][j][k][l][0]))]\n",
    "                    simulated_echo_area = trapz(simulated_echo, dx = 1) \n",
    "                    norm_sim_echo = [x/simulated_echo_area for x in simulated_echo]\n",
    "                    data_arr.append(norm_sim_echo)\n",
    "                    label_arr.append(str(sim_echo_params))\n",
    "                    int_label_arr.append(counter)#+j*range(len(sim_echoes[0][:][:][:])) k*range(len(sim_echoes[0][0][:][:]))+l*range(len(sim_echoes[0][0][0][:]))))\n",
    "                    counter+=1       \n",
    "    for i in range(len(sim_echoes_3[:][:][:][:])): #snow depth{s} (h_s)\n",
    "        for j in range(len(sim_echoes_3[0][:][:][:])): #l_surf{s}\n",
    "            for k in range(len(sim_echoes_3[0][0][:][:])): #sigma_si{s}\n",
    "                for l in range(len(sim_echoes_3[0][0][0][:])): #sigma_surf{s}\n",
    "                    simulated_echo = [sim_echoes_3[i][j][k][l][m][0] for m in range(len(sim_echoes_3[i][j][k][l]))]\n",
    "                    sim_echo_params = [sim_params_3[i][j][k][l][0][m] for m in range(len(sim_params_3[i][j][k][l][0]))]\n",
    "                    simulated_echo_area = trapz(simulated_echo, dx = 1) \n",
    "                    norm_sim_echo = [x/simulated_echo_area for x in simulated_echo]\n",
    "                    data_arr.append(norm_sim_echo)\n",
    "                    label_arr.append(str(sim_echo_params))\n",
    "                    int_label_arr.append(counter)#+j*range(len(sim_echoes[0][:][:][:])) k*range(len(sim_echoes[0][0][:][:]))+l*range(len(sim_echoes[0][0][0][:]))))\n",
    "                    counter+=1                \n",
    "\n",
    "    return np.array(data_arr), np.array(label_arr),np.array(int_label_arr)\n",
    "feature_arr,label_arr,int_label_arr = extractNLP1() #for features for both the SVC and neural network, labels for the SVC and for labels for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9317, 139)\n",
      "(9317,)\n",
      "(9317,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(feature_arr)) #for features for both the SVC and neural network\n",
    "print(np.shape(label_arr)) #for labels for the SVC\n",
    "label_arr = np.array(label_arr, dtype = 'str')\n",
    "print(np.shape(int_label_arr)) #for labels for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(gamma='auto') #creating the suppport vector classification\n",
    "clf.fit(feature_arr, label_arr) #training the SVC using the simulated echoes and their corresponding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7360515021459227"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding NLP3 to the model as well: (total echoes = 5929) (another 2591 echoes):\n",
    "def proportion_correct_predictions():\n",
    "    correct_counter = 0\n",
    "    incorrect_counter = 0\n",
    "    for i in prange(np.shape(feature_arr)[0]):\n",
    "        if i%20 ==0:\n",
    "            #extracting an echo\n",
    "            echo = feature_arr[i]\n",
    "            # slightly modifying the echo (for subsequent denoising accuracy calculation)\n",
    "            echo_mod = [np.random.normal(loc = echo[j], scale = echo[j]*0.05)  if j%5==0 else echo[j] for j in range(len(echo))]\n",
    "            #checking if the modified echo is accurately predicted as the clean echo:\n",
    "            if clf.predict([echo])[0] == clf.predict([echo_mod])[0]:\n",
    "                #counting the number of modified echoes correctly denoised:\n",
    "                correct_counter+=1\n",
    "            else:\n",
    "                #counting the number of modified echoes incorrectly denoised:\n",
    "                incorrect_counter+=1\n",
    "    #returning the proportion of correct predictions:\n",
    "    return correct_counter/(incorrect_counter+correct_counter)\n",
    "proportion_correct_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding NLP3 to the model as well: (total echoes = 5929) (another 2591 echoes):\n",
    "def proportion_correct_predictions():\n",
    "    correct_counter = 0\n",
    "    incorrect_counter = 0\n",
    "    for i in prange(np.shape(feature_arr)[0]):\n",
    "        \n",
    "        #extracting an echo\n",
    "        echo = feature_arr[i]\n",
    "        # slightly modifying the echo (for subsequent denoising accuracy calculation)\n",
    "        echo_mod = [np.random.normal(loc = echo[j], scale = echo[j]*0.025) for j in range(len(echo))]\n",
    "        #checking if the modified echo is accurately predicted as the clean echo:\n",
    "        if clf.predict([echo])[0] == clf.predict([echo_mod])[0]:\n",
    "            #counting the number of modified echoes correctly denoised:\n",
    "            correct_counter+=1\n",
    "        else:\n",
    "            #counting the number of modified echoes incorrectly denoised:\n",
    "            incorrect_counter+=1\n",
    "    #returning the proportion of correct predictions:\n",
    "    return correct_counter/(incorrect_counter+correct_counter)\n",
    "proportion_correct_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo = feature_arr[1000]\n",
    "echo_mod = [np.random.normal(loc = echo[j], scale = echo[j]*0.025) for j in range(len(echo))]\n",
    "plt.plot(echo, label = 'original echo')\n",
    "plt.plot(echo_mod, label = 'corrupted echo')\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylabel('Normalised Power', fontsize = 15)\n",
    "plt.xlabel('range bin number', fontsize = 15)\n",
    "plt.savefig(\"EchoCorruptionErr2.5.png\", dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a CryoSat-2 echo from extracting_waveforms.ipynb file:\n",
    "pickle_in = open(\"file.pickle\", 'rb')\n",
    "obs_arr = pickle.load(pickle_in)\n",
    "clf.predict([obs_arr])\n",
    "# And so the random CryoSat-2 echo is predicted to have a snow depth of 5cm, large-scale correlation length of 8cm, \n",
    "#  a sea ice rms height of 0.0015m and a large-scale rms roughness height of 0.5m. This is promising, as these values\n",
    "#  lie close to the default values when compared to the range in values of the parameters that the SVC trained for,\n",
    "# apart from sigma_surf, which lies on the edge of the parameter space. This has to be taken with some speculation however,\n",
    "#  as it has been assumed that this echo doesnt contain a lead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Lead Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP1 = scipy.io.loadmat('/Users/claudederijkethomas/Desktop/Ice/Proj/FEM_Simulations_temp_Claude.mat')\n",
    "sim_params = LP1['parameters_lookup']\n",
    "sim_echoes = LP1['P_t_ml_range']\n",
    "print(sim_echoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLP1():\n",
    "    counter = 0\n",
    "    data_arr = []\n",
    "    label_arr = []\n",
    "    int_label_arr = []\n",
    "    for i in range(len(sim_echoes[:][:][:][:])): #snow depth{s} (h_s)\n",
    "        for j in range(len(sim_echoes[0][:][:][:])): #l_surf{s}\n",
    "            for k in range(len(sim_echoes[0][0][:][:])): #sigma_si{s}\n",
    "                for l in range(len(sim_echoes[0][0][0][:])): #sigma_surf{s}\n",
    "                    simulated_echo = [sim_echoes[i][j][k][l][m][0] for m in range(len(sim_echoes[i][j][k][l]))]\n",
    "                    sim_echo_params = [sim_params[i][j][k][l][0][m] for m in range(len(sim_params[i][j][k][l][0]))]\n",
    "                    simulated_echo_area = trapz(simulated_echo, dx = 1) \n",
    "                    norm_sim_echo = [x/simulated_echo_area for x in simulated_echo]\n",
    "                    data_arr.append(norm_sim_echo)\n",
    "                    label_arr.append(str(sim_echo_params))\n",
    "                    int_label_arr.append(counter)#+j*range(len(sim_echoes[0][:][:][:])) k*range(len(sim_echoes[0][0][:][:]))+l*range(len(sim_echoes[0][0][0][:]))))\n",
    "                    counter+=1\n",
    "    return np.array(data_arr), np.array(label_arr),np.array(int_label_arr)\n",
    "feature_arr,label_arr,int_label_arr = extractLP1() #for features for both the SVC and neural network, labels for the SVC and for labels for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(feature_arr))\n",
    "print(np.shape(label_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_arr = np.array(label_arr, dtype = 'str')\n",
    "clf = SVC(gamma='auto') #creating the suppport vector classification\n",
    "clf.fit(feature_arr, label_arr) #training the SVC using the simulated echoes and their corresponding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_correct_predictions():\n",
    "    correct_counter = 0\n",
    "    incorrect_counter = 0\n",
    "    for i in prange(np.shape(feature_arr)[0]):\n",
    "        \n",
    "        #extracting an echo\n",
    "        echo = feature_arr[i]\n",
    "        # slightly modifying the echo (for subsequent denoising accuracy calculation)\n",
    "        echo_mod = [np.random.normal(loc = echo[j], scale = echo[j]*0.05)  if j%5==0 else echo[j] for j in range(len(echo))]\n",
    "        #checking if the modified echo is accurately predicted as the clean echo:\n",
    "        if clf.predict([echo])[0] == clf.predict([echo_mod])[0]:\n",
    "            #counting the number of modified echoes correctly denoised:\n",
    "            correct_counter+=1\n",
    "        else:\n",
    "            #counting the number of modified echoes incorrectly denoised:\n",
    "            incorrect_counter+=1\n",
    "    #returning the proportion of correct predictions:\n",
    "    return correct_counter/(incorrect_counter+correct_counter)\n",
    "proportion_correct_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def proportion_correct_predictions():\n",
    "    correct_counter = 0\n",
    "    incorrect_counter = 0\n",
    "    for i in prange(np.shape(feature_arr)[0]):\n",
    "        \n",
    "        #extracting an echo\n",
    "        echo = feature_arr[i]\n",
    "        # slightly modifying the echo (for subsequent denoising accuracy calculation)\n",
    "        echo_mod =[]\n",
    "        for j in prange(len(echo)):\n",
    "            if j%5==0:\n",
    "                np.append(echo_mod, np.random.normal(loc = echo[j], scale = echo[j]*0.05))\n",
    "            else:\n",
    "                np.append(echo_mod, echo[j])\n",
    "\n",
    "        #checking if the modified echo is accurately predicted as the clean echo:\n",
    "        if clf.predict([echo])[0] == clf.predict([echo_mod])[0]:\n",
    "            #counting the number of modified echoes correctly denoised:\n",
    "            correct_counter+=1\n",
    "        else:\n",
    "            #counting the number of modified echoes incorrectly denoised:\n",
    "            incorrect_counter+=1\n",
    "    #returning the proportion of correct predictions:\n",
    "    return correct_counter/(incorrect_counter+correct_counter)\n",
    "proportion_correct_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neural Network (Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = to_categorical(int_label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #adding 400 nodes to the shallow layer:\n",
    "    model.add(Dense(400, input_dim=int(139), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Another hidden layer of 16 units\n",
    "    model.add(Dense(400, kernel_initializer='normal', activation='relu'))\n",
    "    #output layer\n",
    "    model.add(Dense(3388, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our Keras model in an estimator compatible with scikit_learn\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=30, verbose=0)\n",
    "# Now we can use scikit_learn's cross_val_score to evaluate this model identically to the others\n",
    "cv_scores = cross_val_score(estimator, feature_arr, one_hot_labels, cv=15) # cv = train test split (k-1: 1)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking another approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfNN = MLPClassifier(solver='adam', alpha=1e-2, hidden_layer_sizes=(400, 16), random_state=1)\n",
    "clfNN.fit(feature_arr, one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(np.array(clfNN.predict([last_echo])[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
